# ğŸš€ Rolling_project_part_2
This is the second part of the rolling project  

---

# ğŸŒ AWS Flask Viewer
A simple Flask web application packaged in a Docker container.  
It uses **boto3** to connect to AWS and displays the following resources in an HTML page:
- ğŸ–¥ï¸ EC2 instances  
- ğŸŒ‰ VPCs  
- âš–ï¸ Load Balancers  
- ğŸ“¸ AMIs  

The application runs on **port 5001**.

---

## âœ… Prerequisites
- ğŸ³ Docker installed  
- ğŸ”‘ AWS IAM user with:  
  - Access Key ID  
  - Secret Access Key  
- ğŸ‘® Permissions to list EC2, VPC, ELB, and AMI resources (for testing, `AdministratorAccess` is sufficient)  

---

## ğŸ› ï¸ Instructions

### ğŸ› ï¸ Build Locally
```bash
docker build -t project-flask .
```
## ğŸ“¦ Pull from Docker Hub
```bash
docker pull cheeza42/dockerizing-project:v1
```
**View on Docker Hub:**
 https://hub.docker.com/r/cheeza42/dockerizing-project


## â–¶ï¸ Run the Container on AWS
 ```bash
docker run -d --name project-flask \
  -p 5001:5001 \
  -e AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID \
  -e AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY \
  -e AWS_DEFAULT_REGION=eu-west-1 \
  cheeza42/dockerizing-project:v1
 ```
## ğŸŒ Access the Application
Open your browser at:
ğŸ‘‰ http://localhost:5001/


## ğŸ›‘ Stop and Remove the Container
```bash
docker stop project-flask
docker rm project-flask
```

## âˆ Deploy on Kubernetes with Helm

This repo also includes a Helm chart (chart/helm_chart_projct) to deploy the app to Kubernetes.

## ğŸ“‹ Prerequisites

Kubernetes cluster with kubectl configured

Helm v3+

NGINX Ingress Controller (required because ingress.enabled: true in values.yaml)

## ğŸš€ Install
```bash
helm install aws-viewer ./helm_chart_projct
kubectl get pods,svc,ingress
```

## ğŸŒ Access the app
Ingress (enabled)

According to values.yaml:

- ingress.enabled: true

- ingress.ingressClassName: nginx

- ingress.host: awsviewer.com

So the app will be available at:

http://awsviewer.com/


## âš ï¸ DNS for awsviewer.com must resolve to your Ingress controllerâ€™s IP.
For local testing (e.g., minikube), add an entry in /etc/hosts:
<INGRESS-IP> awsviewer.com

## Fallback: ClusterIP (port-forward)

If Ingress is not working, the Service is ClusterIP on port 80 â†’ targetPort 5001.
You can forward locally:
```bash
kubectl port-forward svc/aws-viewer 8080:80
```
 # Then open http://localhost:8080/

## ğŸ”„ Upgrade / Uninstall
```bash
helm upgrade aws-viewer ./helm_chart_projct -f helm_chart_projct/values.yaml
helm uninstall aws-viewer
```
## âš™ï¸ CI/CD Automation with Jenkins & Kaniko

This project integrates **Jenkins** and **Kaniko** to automate the entire build and push process of the Docker image to Docker Hub â€” without requiring Docker to run inside the Jenkins agent.  

### ğŸ§© Pipeline Overview
The Jenkins pipeline performs the following stages:

1. **Clone Repository** â€“ Pulls the latest code from GitHub.  
2. **Compute Tag** â€“ Generates a unique image tag based on date and commit SHA.  
3. **Parallel Checks** â€“  
   - *Linting:* Runs `flake8`, `hadolint`, and `shellcheck`.  
   - *Security Scanning:* Runs `bandit` and `trivy`.  
4. **Build & Push with Kaniko** â€“  
   Uses the Kaniko executor container to build the Docker image from the `Dockerfile` and push it directly to Docker Hub.

### â˜¸ï¸ Environment
The Jenkins pipeline runs inside a **Kubernetes Pod**, where:
- The **Kaniko container** handles image building and pushing to Docker Hub.  
- The **Jenkins agent container** manages pipeline orchestration and log output.

### ğŸ§± Jenkinsfile Reference
The complete pipeline implementation can be found in the project root under:  
ğŸ“„ **[`jenkinsfile`](./jenkinsfile)**  

It defines all stages, environment variables, and credentials used for the automated CI/CD workflow.

### ğŸ“¦ Result
Once the pipeline completes, the image is automatically pushed to Docker Hub under:  
ğŸ‘‰ **[`cheeza42/dockerizing-project`](https://hub.docker.com/r/cheeza42/dockerizing-project)**


## ğŸ“‘ Chart details

Replicas: 2

Image: cheeza42/dockerizing-project:v1

Service: ClusterIP (port 80 â†’ container port 5001)

Ingress: NGINX, host awsviewer.com

AWS credentials: passed as environment variables

Probes: readiness & liveness on /ready and /health  

Resources: requests (100m CPU, 128Mi RAM), limits (500m CPU, 256Mi RAM) 

Service labels: app: <Release.Name>  

âœ… Recommendation: In production, store AWS credentials in a Kubernetes Secret and reference them from the Deployment.

check check 

trying to make a webhook